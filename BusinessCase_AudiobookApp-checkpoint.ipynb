{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac79dbef",
   "metadata": {},
   "source": [
    "# Business Case: Audiobook App Buyer Behaviour\n",
    "## Data pre-processing\n",
    "\n",
    "\n",
    "Here I analyse buyer data from a audiobook app to inform decision making with regards to the most effective marketting strategy. I use a TensorFlow 2 Machine Learning Approach to predict which customers are likely to make additional purchaces. The insights from this analysis will help prevent the spending of marketting resources on consumers that are unlikely to return to the app.\n",
    "\n",
    "The data for this analysis was collected via the app and represents two years of enagement data. Whether the customer made another audiobook purchace via the app during the 6 months after the 2 year period was then recorded.\n",
    "\n",
    "Variables overview:  \n",
    "Customer ID  \n",
    "Book length  \n",
    "Book length  \n",
    "Prive_overall  \n",
    "Price_avg  \n",
    "Review  \n",
    "Review 10/10 (all empty values replaced with average review rating - 8.91. Thus anything above this number equates to above average feelings, and everything below to below average feelings)  \n",
    "Minutes_listened  \n",
    "Completion (total minutes listened/book length_overall)  \n",
    "Support Requests(measure of engagement)  \n",
    "Last vissited minus Purchase date (measure of engagement)  \n",
    "Targets (whether customer made another purchace during the 6 month period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5b166",
   "metadata": {},
   "source": [
    "1) Preprocess the data  \n",
    "   - Balance the dataset  \n",
    "   - Divide the dataset in training, validation and test  \n",
    "   - Save the data in a tensor friendly format (.npz)  \n",
    "\n",
    "2) Create the Machine learning algorithm   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f8377",
   "metadata": {},
   "source": [
    "### Extract data (inputs and targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fefff54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\christiaan.brink\\\\OneDrive\\\\Github\\\\BusinessCase_Audiobooks'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\christiaan.brink\\\\OneDrive\\\\Github\\\\BusinessCase_Audiobooks\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cf7ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sklearn preprocessing library - for easier standardization of the data.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Load the data\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "# First column [:, 0] is an arbitrary customer ID that can be removed as it contains no useful information for our model.\n",
    "# The last column [:, -1] contains our targets (whether the customer made another purchace during our 6 months period)\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e407d3",
   "metadata": {},
   "source": [
    "### Balance the data sheet\n",
    "Use sklearn capabilities for standardizing the inputs. There are many more \"0\" (not return buyers) than \"1\" (return buyers) in the targets. We need to scale the data so that similar numbers of of \"0\" and \"1\" targets are used to build the model - otherwise the model's solution will be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31d21c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# We want to create a \"balanced\" dataset (where 0 or 1 targets are not over-represented), so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will remove some observations from the over-represented target:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Count the number of targets that are 0. \n",
    "# The for loop below marks entries where the target is 0 once an equal number of 0s as 1s have been identified:\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# Create new variables for both model inputs and targets, while deleting indices marked for removal in the above loop:\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60b887",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52866bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's preprocessing capabilities to standardize the inputs:\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22498dcc",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "\n",
    "This randomizes the order of the data in order to prevent bias during batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7756c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was arranged by date, since the data will be batched it needs to be randomized.\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices above to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d509f4",
   "metadata": {},
   "source": [
    "### Split the dataset (training, test, validation)\n",
    "\n",
    "The data needs to be split into different datasets for training, testing and validation. A 80-10-10 split will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25a6f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797.0 3579 0.5020955574182733\n",
      "217.0 447 0.4854586129753915\n",
      "223.0 448 0.49776785714285715\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Determine split count\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create training data\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create validation data\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create testing data\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# Check that the various datasets are still balanced (50/50 for targets 0 and 1)\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c189e",
   "metadata": {},
   "source": [
    "### Save output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fd26f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "# In the next lesson, you will see that it is extremely valuable to name them in such a coherent way!\n",
    "\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)\n",
    "\n",
    "# Note due to randomization output files will be different each time this code is rerun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
